{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Connector using Apache Arrow Format\n",
    "The Snowflake Connector for Spark (“Spark Connector”) now uses the Apache Arrow \n",
    "columnar result format to dramatically improve query read performance.\n",
    "\n",
    "With this 2.6.0 release, the Snowflake Spark Connector executes the query directly \n",
    "via JDBC and (de)serializes the data using Arrow, Snowflake’s new client result \n",
    "format. This saves time in data reads and also enables the use of cached query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Enter the values of account, user, password, warehouse and role to match your environment\n",
    "val ACCOUNT = \"<snowflake_account>.snowflakecomputing.com\"\n",
    "val USER =  \"<snowflake_user_name>\"\n",
    "val PASSWORD =  \"<snowflake_password>\"\n",
    "val WAREHOUSE = \"<snowflake_warehouse>\"\n",
    "val DATABASE = \"SNOWFLAKE_SAMPLE_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Snowflake Spark Connector options\n",
    "// The default value of \"use_cached_result\" is \"false\".\n",
    "// It is \"true\" means to disable this feature.\n",
    "import net.snowflake.spark.snowflake._\n",
    "\n",
    "val sfOptions: Map[String, String] = Map(\n",
    "\"use_copy_unload\" -> \"false\",  \n",
    "\"use_cached_result\" -> \"false\",\n",
    "\"partition_size_in_mb\" -> \"60\",   \n",
    "\"sfSSL\" -> \"on\",\n",
    "\"sfUser\" -> USER,\n",
    "\"sfPassword\" -> PASSWORD,\n",
    "\"sfDatabase\" -> DATABASE,\n",
    "\"sfURL\" -> ACCOUNT,\n",
    "\"sfWarehouse\" -> WAREHOUSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val sqlContext = new SQLContext(sc) \n",
    "// Test table is TPCH LINEITEM which has 600M rows.\n",
    "// Its compressed size in Snowflake is 16.3GB.\n",
    "val sourceTableName = \"LINEITEM\"\n",
    "val sourceSchema = \"TPCH_SF100\"\n",
    "\n",
    "val df = sqlContext.read.format(\"net.snowflake.spark.snowflake\")\n",
    ".options(sfOptions)\n",
    ".option(\"dbtable\", sourceTableName)\n",
    ".option(\"sfSchema\", sourceSchema)\n",
    ".load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Clear cache before any test\n",
    "sqlContext.clearCache()\n",
    "// Execute cache() and show(1) to read all data into cache\n",
    "// and show one row. The time to show one row can be ignored.\n",
    "// The DataFrame execution time is regarded as the reading time.\n",
    "val startTime = System.currentTimeMillis()\n",
    "df.cache().show(1)\n",
    "val endTime = System.currentTimeMillis()\n",
    "\n",
    "val result_msg = s\"read time: ${(endTime - startTime).toDouble/1000.0} s\"\n",
    "println(result_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
